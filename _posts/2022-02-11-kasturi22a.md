---
title: Text Ranking and Classification using Data Compression
abstract: A well-known but rarely used approach to text categorization uses conditional
  entropy estimates computed using data compression tools. Text affinity scores derived
  from compressed sizes can be used for classification and ranking tasks, but their
  success depends on the compression tools used. We use the Zstandard compressor and
  strengthen these ideas in several ways, calling the resulting language-agnostic
  technique Zest. In applications, this approach simplifies configuration, avoiding
  careful feature extraction and large ML models. Our ablation studies confirm the
  value of individual enhancements we introduce. We show that Zest complements and
  can compete with language-specific multidimensional content embeddings in production,
  but cannot outperform other counting methods on public datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kasturi22a
month: 0
tex_title: Text Ranking and Classification using Data Compression
firstpage: 48
lastpage: 53
page: 48-53
order: 48
cycles: false
bibtex_author: Kasturi, Nitya and Markov, Igor L.
author:
- given: Nitya
  family: Kasturi
- given: Igor L.
  family: Markov
date: 2022-02-11
address:
container-title: Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS
  2021 Workshops
volume: '163'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 2
  - 11
pdf: https://proceedings.mlr.press/v163/kasturi22a/kasturi22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
