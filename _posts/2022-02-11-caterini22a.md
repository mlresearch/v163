---
title: Entropic Issues in Likelihood-Based OOD Detection
abstract: Deep generative models trained by maximum likelihood remain very popular
  methods for reasoning about data probabilistically. However, it has been observed
  that they can assign higher likelihoods to out-of-distribution (OOD) data than in-
  distribution data, thus calling into question the meaning of these likelihood values.
  In this work we provide a novel perspective on this phenomenon, decomposing the
  average likelihood into a KL divergence term and an entropy term. We argue that
  the latter can explain the curious OOD behaviour mentioned above, suppressing likelihood
  values on datasets with higher entropy. Although our idea is simple, we have not
  seen it explored yet in the literature. This analysis provides further explanation
  for the success of OOD detection methods based on likelihood ratios, as the problematic
  entropy term cancels out in expectation. Finally, we discuss how this observation
  relates to recent success in OOD detection with manifold-supported models, for which
  the above decomposition does not hold.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: caterini22a
month: 0
tex_title: Entropic Issues in Likelihood-Based {OOD} Detection
firstpage: 21
lastpage: 26
page: 21-26
order: 21
cycles: false
bibtex_author: Caterini, Anthony L. and Loaiza-Ganem, Gabriel
author:
- given: Anthony L.
  family: Caterini
- given: Gabriel
  family: Loaiza-Ganem
date: 2022-02-11
address:
container-title: Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS
  2021 Workshops
volume: '163'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 2
  - 11
pdf: https://proceedings.mlr.press/v163/caterini22a/caterini22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
