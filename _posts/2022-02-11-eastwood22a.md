---
title: Unit-level surprise in neural networks
abstract: 'To adapt to changes in real-world data distributions, neural networks must
  update their parameters. We argue that unit-level surprise should be useful for:
  (i) determining which few parameters should update to adapt quickly; and (ii) learning
  a modularization such that few modules need be adapted to transfer. We empirically
  validate (i) in simple settings and reflect on the challenges and opportunities
  of realizing both (i) and (ii) in more general settings.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: eastwood22a
month: 0
tex_title: Unit-level surprise in neural networks
firstpage: 33
lastpage: 40
page: 33-40
order: 33
cycles: false
bibtex_author: Eastwood, Cian and Mason, Ian and Williams, Christopher K. I.
author:
- given: Cian
  family: Eastwood
- given: Ian
  family: Mason
- given: Christopher K. I.
  family: Williams
date: 2022-02-11
address:
container-title: Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS
  2021 Workshops
volume: '163'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 2
  - 11
pdf: https://proceedings.mlr.press/v163/eastwood22a/eastwood22a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v163/eastwood22a/eastwood22a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
