---
title: The Curse of Depth in Kernel Regime
abstract: Recent work by Jacot et al. (2018) has shown that training a neural network
  of any kind with gradient descent is strongly related to kernel gradient descent
  in function space with respect to the Neural Tangent Kernel (NTK). Empirical results
  in (Lee et al., 2019) demonstrated high performance of a linearized version of training
  using the so-called NTK regime. In this paper, we show that the large depth limit
  of this regime is unexpectedly trivial, and we fully characterize the convergence
  rate to this trivial regime.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hayou22a
month: 0
tex_title: The Curse of Depth in Kernel Regime
firstpage: 41
lastpage: 47
page: 41-47
order: 41
cycles: false
bibtex_author: Hayou, Soufiane and Doucet, Arnaud and Rousseau, Judith
author:
- given: Soufiane
  family: Hayou
- given: Arnaud
  family: Doucet
- given: Judith
  family: Rousseau
date: 2022-02-11
address:
container-title: Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS
  2021 Workshops
volume: '163'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 2
  - 11
pdf: https://proceedings.mlr.press/v163/hayou22a/hayou22a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v163/hayou22a/hayou22a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
