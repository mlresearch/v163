---
title: Is the Number of Trainable Parameters All That Actually Matters?
abstract: 'Recent work has identified simple empirical scaling laws for language models,
  linking compute budget, dataset size, model size, and autoregressive modeling loss.
  The validity of these simple power laws across orders of magnitude in model scale
  provides compelling evidence that larger models are also more capable models. However,
  scaling up models under the constraints of hardware and infrastructure is no easy
  feat, and rapidly becomes a hard and expensive engineering problem. We investigate
  ways to tentatively cheat scaling laws, and train larger models for cheaper. We
  emulate an increase in effective parameters, using efficient approximations: either
  by doping the models with frozen random parameters, or by using fast structured
  transforms in place of dense linear layers. We find that the scaling relationship
  between test loss and compute depends only on the actual number of trainable parameters;
  scaling laws cannot be deceived by spurious parameters.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chatelain22a
month: 0
tex_title: Is the Number of Trainable Parameters All That Actually Matters?
firstpage: 27
lastpage: 32
page: 27-32
order: 27
cycles: false
bibtex_author: Chatelain, Am\'{e}lie and Djeghri, Amine and Hesslow, Daniel and Launay,
  Julien
author:
- given: Am√©lie
  family: Chatelain
- given: Amine
  family: Djeghri
- given: Daniel
  family: Hesslow
- given: Julien
  family: Launay
date: 2022-02-11
address:
container-title: Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS
  2021 Workshops
volume: '163'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 2
  - 11
pdf: https://proceedings.mlr.press/v163/chatelain22a/chatelain22a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v163/chatelain22a/chatelain22a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
